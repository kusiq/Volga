# Volga Generative pre-trained transformer

![Frame 1 (1)](https://github.com/user-attachments/assets/3f0a29d9-ff68-48c7-94f6-cdfc5c243d89)

# Volga Alpha model - v0.3 KROT (WIP)

Языковая модель "Vолга" доученая на базе GPT2 (137.000.000 параметров). Обучена на содержании 0.5-1.0% русскоязычной википедии, а датасет по сравнению с v0.2 увеличен в два раза - до 1 миллиона символов.
Сможет отвечать более развернуто и разнообразно в отличие от "Белки", которая была простым генератором продолжения стиха.

Кодовое название версии 0.3 - "Крот". Имеет возможность дать ответ на вопрос.
