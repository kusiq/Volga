# Volga Generative pre-trained transformer

![Frame 1 (1)](https://github.com/user-attachments/assets/3f0a29d9-ff68-48c7-94f6-cdfc5c243d89)

# Volga Alpha model - v0.3 KROT (WIP)

Языковая модель "Vолга" доученая на базе GPT2 (137.000.000 параметров). Обучена на содержании 0.5-1.0% русскоязычной википедии, а датасет по сравнению с v0.2 увеличен в два раза - до 1 миллиона символов.
Сможет отвечать более развернуто и разнообразно в отличие от "Белки", которая была простым генератором продолжения стиха.

Кодовое название версии 0.3 - "Крот". Имеет возможность дать ответ на вопрос.

# Последующие обновления

- Проверка на возможные ошибки перед началом обучения, чтобы после завершения/во время обучения ничего не сломалось и не убило часы и часы времени.
- Дообучение моделей на основе предыдущих обучений, чтобы не терять время на повторное обучение информации
- Интегрировать в UI: сайты, телеграм
